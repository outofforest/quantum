// Code generated by command: go run avo.go -out ../asm.s -stubs ../asm_stub.go -pkg checksum. DO NOT EDIT.

#include "textflag.h"

// func Transpose(x **uint32, z *uint32)
// Requires: AVX512F
TEXT ·Transpose(SB), NOSPLIT, $0-16
	MOVQ       x+0(FP), AX
	MOVQ       z+8(FP), CX
	MOVQ       (AX), DX
	VMOVDQA64  (DX), Z0
	MOVQ       8(AX), DX
	VMOVDQA64  (DX), Z1
	MOVQ       16(AX), DX
	VMOVDQA64  (DX), Z2
	MOVQ       24(AX), DX
	VMOVDQA64  (DX), Z3
	MOVQ       32(AX), DX
	VMOVDQA64  (DX), Z4
	MOVQ       40(AX), DX
	VMOVDQA64  (DX), Z5
	MOVQ       48(AX), DX
	VMOVDQA64  (DX), Z6
	MOVQ       56(AX), DX
	VMOVDQA64  (DX), Z7
	MOVQ       64(AX), DX
	VMOVDQA64  (DX), Z8
	MOVQ       72(AX), DX
	VMOVDQA64  (DX), Z9
	MOVQ       80(AX), DX
	VMOVDQA64  (DX), Z10
	MOVQ       88(AX), DX
	VMOVDQA64  (DX), Z11
	MOVQ       96(AX), DX
	VMOVDQA64  (DX), Z12
	MOVQ       104(AX), DX
	VMOVDQA64  (DX), Z13
	MOVQ       112(AX), DX
	VMOVDQA64  (DX), Z14
	MOVQ       120(AX), DX
	VMOVDQA64  (DX), Z15
	VSHUFPS    $0x44, Z1, Z0, Z16
	VSHUFPS    $0xee, Z1, Z0, Z17
	VSHUFPS    $0x44, Z3, Z2, Z1
	VSHUFPS    $0xee, Z3, Z2, Z3
	VSHUFPS    $0x44, Z5, Z4, Z23
	VSHUFPS    $0xee, Z5, Z4, Z18
	VSHUFPS    $0x44, Z7, Z6, Z24
	VSHUFPS    $0xee, Z7, Z6, Z7
	VSHUFPS    $0x44, Z9, Z8, Z19
	VSHUFPS    $0xee, Z9, Z8, Z20
	VSHUFPS    $0x44, Z11, Z10, Z9
	VSHUFPS    $0xee, Z11, Z10, Z11
	VSHUFPS    $0x44, Z13, Z12, Z21
	VSHUFPS    $0xee, Z13, Z12, Z22
	VSHUFPS    $0x44, Z15, Z14, Z25
	VSHUFPS    $0xee, Z15, Z14, Z15
	VSHUFPS    $0x88, Z1, Z16, Z0
	VSHUFPS    $0xdd, Z1, Z16, Z2
	VSHUFPS    $0x88, Z3, Z17, Z4
	VSHUFPS    $0xdd, Z3, Z17, Z6
	VSHUFPS    $0x88, Z24, Z23, Z1
	VSHUFPS    $0xdd, Z24, Z23, Z3
	VSHUFPS    $0x88, Z7, Z18, Z5
	VSHUFPS    $0xdd, Z7, Z18, Z7
	VSHUFPS    $0x88, Z9, Z19, Z8
	VSHUFPS    $0xdd, Z9, Z19, Z10
	VSHUFPS    $0x88, Z11, Z20, Z12
	VSHUFPS    $0xdd, Z11, Z20, Z14
	VSHUFPS    $0x88, Z25, Z21, Z9
	VSHUFPS    $0xdd, Z25, Z21, Z11
	VSHUFPS    $0x88, Z15, Z22, Z13
	VSHUFPS    $0xdd, Z15, Z22, Z15
	VSHUFI32X4 $0x44, Z1, Z0, Z16
	VSHUFI32X4 $0xee, Z1, Z0, Z19
	VSHUFI32X4 $0x44, Z3, Z2, Z17
	VSHUFI32X4 $0xee, Z3, Z2, Z20
	VSHUFI32X4 $0x44, Z5, Z4, Z23
	VSHUFI32X4 $0xee, Z5, Z4, Z21
	VSHUFI32X4 $0x44, Z7, Z6, Z18
	VSHUFI32X4 $0xee, Z7, Z6, Z22
	VSHUFI32X4 $0x44, Z9, Z8, Z1
	VSHUFI32X4 $0xee, Z9, Z8, Z9
	VSHUFI32X4 $0x44, Z11, Z10, Z3
	VSHUFI32X4 $0xee, Z11, Z10, Z11
	VSHUFI32X4 $0x44, Z13, Z12, Z24
	VSHUFI32X4 $0xee, Z13, Z12, Z25
	VSHUFI32X4 $0x44, Z15, Z14, Z7
	VSHUFI32X4 $0xee, Z15, Z14, Z15
	VSHUFI32X4 $0x88, Z1, Z16, Z0
	VSHUFI32X4 $0xdd, Z1, Z16, Z4
	VSHUFI32X4 $0x88, Z3, Z17, Z1
	VSHUFI32X4 $0xdd, Z3, Z17, Z5
	VSHUFI32X4 $0x88, Z24, Z23, Z2
	VSHUFI32X4 $0xdd, Z24, Z23, Z6
	VSHUFI32X4 $0x88, Z7, Z18, Z3
	VSHUFI32X4 $0xdd, Z7, Z18, Z7
	VSHUFI32X4 $0x88, Z9, Z19, Z8
	VSHUFI32X4 $0xdd, Z9, Z19, Z12
	VSHUFI32X4 $0x88, Z11, Z20, Z9
	VSHUFI32X4 $0xdd, Z11, Z20, Z13
	VSHUFI32X4 $0x88, Z25, Z21, Z10
	VSHUFI32X4 $0xdd, Z25, Z21, Z14
	VSHUFI32X4 $0x88, Z15, Z22, Z11
	VSHUFI32X4 $0xdd, Z15, Z22, Z15
	VMOVDQA64  Z0, (CX)
	VMOVDQA64  Z1, 64(CX)
	VMOVDQA64  Z2, 128(CX)
	VMOVDQA64  Z3, 192(CX)
	VMOVDQA64  Z4, 256(CX)
	VMOVDQA64  Z5, 320(CX)
	VMOVDQA64  Z6, 384(CX)
	VMOVDQA64  Z7, 448(CX)
	VMOVDQA64  Z8, 512(CX)
	VMOVDQA64  Z9, 576(CX)
	VMOVDQA64  Z10, 640(CX)
	VMOVDQA64  Z11, 704(CX)
	VMOVDQA64  Z12, 768(CX)
	VMOVDQA64  Z13, 832(CX)
	VMOVDQA64  Z14, 896(CX)
	VMOVDQA64  Z15, 960(CX)
	RET

// func G(a *[16]uint32, b *[16]uint32, c *[16]uint32, d *[16]uint32, mx *[16]uint32, my *[16]uint32)
// Requires: AVX512F
TEXT ·G(SB), NOSPLIT, $0-48
	MOVQ      a+0(FP), AX
	MOVQ      b+8(FP), CX
	MOVQ      c+16(FP), DX
	MOVQ      d+24(FP), BX
	VMOVDQA64 (AX), Z0
	VMOVDQA64 (CX), Z1
	VMOVDQA64 (DX), Z2
	VMOVDQA64 (BX), Z3
	MOVQ      mx+32(FP), SI
	VMOVDQA64 (SI), Z4
	MOVQ      my+40(FP), SI
	VMOVDQA64 (SI), Z5
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z4, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x10, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x0c, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z5, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x08, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x07, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z4, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x10, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x0c, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z5, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x08, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x07, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z4, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x10, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x0c, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z5, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x08, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x07, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z4, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x10, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x0c, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z5, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x08, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x07, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z4, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x10, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x0c, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z5, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x08, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x07, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z4, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x10, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x0c, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z5, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x08, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x07, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z4, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x10, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x0c, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z5, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x08, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x07, Z1, Z1
	VMOVDQA64 Z0, (AX)
	VMOVDQA64 Z1, (CX)
	VMOVDQA64 Z2, (DX)
	VMOVDQA64 Z3, (BX)
	RET

// func Add(x *[16]uint32, y *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·Add(SB), NOSPLIT, $0-24
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	MOVQ      y+8(FP), AX
	VMOVDQA64 (AX), Z1
	VPADDD    Z0, Z1, Z0
	MOVQ      z+16(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func Add10(x *[16]uint32, y *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·Add10(SB), NOSPLIT, $0-24
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	MOVQ      y+8(FP), AX
	VMOVDQA64 (AX), Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	MOVQ      z+16(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func Xor(x *[16]uint32, y *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·Xor(SB), NOSPLIT, $0-24
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	MOVQ      y+8(FP), AX
	VMOVDQA64 (AX), Z1
	VPXORD    Z0, Z1, Z0
	MOVQ      z+16(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func Xor10(x *[16]uint32, y *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·Xor10(SB), NOSPLIT, $0-24
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	MOVQ      y+8(FP), AX
	VMOVDQA64 (AX), Z1
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	MOVQ      z+16(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight7(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight7(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x07, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight107(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight107(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight8(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight8(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x08, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight108(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight108(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight12(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight12(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x0c, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight1012(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight1012(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight16(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight16(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x10, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight1016(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight1016(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET
