// Code generated by command: go run avo.go -out ../asm.s -stubs ../asm_stub.go -pkg checksum. DO NOT EDIT.

#include "textflag.h"

// func G(a *[16]uint32, b *[16]uint32, c *[16]uint32, d *[16]uint32, mx *[16]uint32, my *[16]uint32)
// Requires: AVX512F
TEXT ·G(SB), NOSPLIT, $0-48
	MOVQ      mx+32(FP), AX
	VMOVDQA64 (AX), Z4
	MOVQ      my+40(FP), AX
	VMOVDQA64 (AX), Z5
	MOVQ      a+0(FP), AX
	MOVQ      b+8(FP), CX
	MOVQ      c+16(FP), DX
	MOVQ      d+24(FP), BX
	VMOVDQA64 (AX), Z0
	VMOVDQA64 (CX), Z1
	VMOVDQA64 (DX), Z2
	VMOVDQA64 (BX), Z3
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z4, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x10, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x0c, Z1, Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z5, Z0
	VPXORD    Z3, Z0, Z3
	VPRORD    $0x08, Z3, Z3
	VPADDD    Z2, Z3, Z2
	VPXORD    Z1, Z2, Z1
	VPRORD    $0x07, Z1, Z1
	VMOVDQA64 Z0, (AX)
	VMOVDQA64 Z1, (CX)
	VMOVDQA64 Z2, (DX)
	VMOVDQA64 Z3, (BX)
	RET

// func Add(x *[16]uint32, y *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·Add(SB), NOSPLIT, $0-24
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	MOVQ      y+8(FP), AX
	VMOVDQA64 (AX), Z1
	VPADDD    Z0, Z1, Z0
	MOVQ      z+16(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func Add10(x *[16]uint32, y *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·Add10(SB), NOSPLIT, $0-24
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	MOVQ      y+8(FP), AX
	VMOVDQA64 (AX), Z1
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	VPADDD    Z0, Z1, Z0
	MOVQ      z+16(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func Xor(x *[16]uint32, y *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·Xor(SB), NOSPLIT, $0-24
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	MOVQ      y+8(FP), AX
	VMOVDQA64 (AX), Z1
	VPXORD    Z0, Z1, Z0
	MOVQ      z+16(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func Xor10(x *[16]uint32, y *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·Xor10(SB), NOSPLIT, $0-24
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	MOVQ      y+8(FP), AX
	VMOVDQA64 (AX), Z1
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	VPXORD    Z0, Z1, Z0
	MOVQ      z+16(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight7(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight7(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x07, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight107(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight107(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	VPRORD    $0x07, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight8(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight8(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x08, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight108(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight108(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	VPRORD    $0x08, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight12(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight12(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x0c, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight1012(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight1012(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	VPRORD    $0x0c, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight16(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight16(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x10, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET

// func RotateRight1016(x *[16]uint32, z *[16]uint32)
// Requires: AVX512F
TEXT ·RotateRight1016(SB), NOSPLIT, $0-16
	MOVQ      x+0(FP), AX
	VMOVDQA64 (AX), Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	VPRORD    $0x10, Z0, Z0
	MOVQ      z+8(FP), AX
	VMOVDQA64 Z0, (AX)
	RET
